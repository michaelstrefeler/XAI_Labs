{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b21a3b17",
   "metadata": {},
   "source": [
    "# XAI 2025 TP5 \n",
    "\n",
    "## PostHoc - Image data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b877a6",
   "metadata": {},
   "source": [
    "Author: Arthur Babey\n",
    "\n",
    "Due: 13.05.2025, 23h59\n",
    "\n",
    "- Professor: Carlos Peña (<a href=\"mailto:carlos.pena@heig-vd.ch\">carlos.pena@heig-vd.ch</a>)\n",
    "- Assistant 2025: Arthur Babey (<a href=\"mailto:arthur.babey@heig-vd.ch\">arthur.babey@heig-vd.ch</a>)\n",
    "\n",
    "\n",
    "Date: Spring 2025\n",
    "\n",
    "* You will need to fill code and answer questions in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03a7fb",
   "metadata": {},
   "source": [
    "You have a folder called **XAI_TP5_2025_MATERIAL** structured like this :\n",
    "\n",
    "**XAI_TP5_2025_MATERIAL**/\n",
    "\n",
    "\tdata/\n",
    "\t\tskin-lesion/\n",
    "            test/\n",
    "                empty : download it             \n",
    "            metadata/\n",
    "                csv with metadata info\n",
    "            label/\n",
    "                csv with label info\n",
    "        \n",
    "\tmodels/\n",
    "            model.pth/\n",
    "                model's weight\n",
    "\n",
    "            train_metrics.csv\n",
    "\n",
    "            confusion_matrix.png            \n",
    "            \n",
    "            \n",
    "    XAI_TP5_2025_Image_NOM_PRENOM.ipynb\n",
    "    \n",
    "    scripts/\n",
    "        scripts used for training, evaluation, customdataset and helpers function\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbcc15f",
   "metadata": {},
   "source": [
    "---\n",
    "- **Link to download the test set** : https://www.swisstransfer.com/d/1d795359-d947-4c90-8b24-1bba007286b0\n",
    "\n",
    "- Place it in ./data/skin-lesion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2e333",
   "metadata": {},
   "source": [
    "---\n",
    "# Instructions\n",
    "\n",
    "During this TP we will use the ISIC 2019 [dataset](https://www.kaggle.com/datasets/andrewmvd/isic-2019). The dataset for ISIC 2019 contains 25,331 images available for the classification of dermoscopic images among nine different diagnostic categories:\n",
    "- Melanoma\n",
    "- Melanocytic nevus\n",
    "- Basal cell carcinoma\n",
    "- Actinic keratosis\n",
    "- Benign keratosis (solar lentigo / seborrheic keratosis / lichen planus-like keratosis)\n",
    "- Dermatofibroma\n",
    "- Vascular lesion\n",
    "- Squamous cell carcinoma\n",
    "- None of the above\n",
    "\n",
    "The dataset was splitted into and train and a test data. The full dataset is accessible on kaggle and the test dataset you will need to use here is stored in ./data/test/\n",
    "\n",
    "You have been provided with a fine-tuned EfficientNet-B3 model for this dataset. EfficientNet-B3 is a convolutional neural network architecture that offers high classification accuracy while requiring reasonable computing power. It was introduced in the paper 'EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks' by Tan et al. (2019). If you are interested, you can find more details about the architecture in the paper by following this [link](https://arxiv.org/abs/1905.11946).\n",
    "\n",
    "We also give you multiple function that will make your life easier to plot, make inference, plot on test images. They are described in ./scripts/helpers.py.\n",
    "\n",
    "---\n",
    "\n",
    "You will need to complete code, generate plot and answer questions\n",
    "\n",
    "\n",
    "Goal :\n",
    "\n",
    "0. Explore : explore the dataset and the model performance\n",
    "1. Use XAI's method to generate explanation \n",
    "2. Expain model's behaviour using PostHoc methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c918df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To install the required libraries, run the following commands:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision\n",
    "pip install captum\n",
    "pip install efficientnet_pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1d00d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthurbabey/miniconda3/envs/ML/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from captum.attr import LayerGradCam\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "from scripts.evaluate import evaluate\n",
    "from scripts.helpers import *\n",
    "from scripts.custom_dataset import CustomImageDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea34f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting paths \n",
    "\n",
    "home_path = os.getcwd()\n",
    "\n",
    "# PATHs \n",
    "\n",
    "data_path = os.path.join(home_path, \"data\", \"skin-lesion\")\n",
    "test_dir_path = os.path.join(data_path, \"test\")\n",
    "label_test_path = os.path.join(data_path, \"label\", \"label_test.csv\")\n",
    "model_path = os.path.join(home_path, \"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016eee38",
   "metadata": {},
   "source": [
    "## 0. Explore dataset and model\n",
    "\n",
    "\n",
    "- First we load the trained model and the test dataset\n",
    "\n",
    "- Then we can evaluate it, the run is quite long so you don't have to run it\n",
    "\n",
    "- The evaluation metrics are shown and a confusion matrix is ploted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f945198c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7p/vsz_gnkx7sg_k1blv9xktznr0000gn/T/ipykernel_3008/3517477962.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(os.path.join(model_path, \"model.pth\"), map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# set device cuda, MPS or cpu\n",
    "device = get_device()\n",
    "\n",
    "# Load model \n",
    "model = EfficientNet.from_pretrained(\"efficientnet-b3\", num_classes=8).to(device)\n",
    "state = torch.load(os.path.join(model_path, \"model.pth\"), map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# load the test set\n",
    "test_dataset = CustomImageDataset(csv_path=label_test_path, data_path=test_dir_path, transform=transforms_eval())\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# class names\n",
    "class_names = [\"MEL\", \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\", \"SCC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b10b7d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8489\n",
      "F1 Score (macro): 0.7252\n",
      "Confusion Matrix:\n",
      "[[ 366  175   14   16    0    6    0    4]\n",
      " [ 127 1951   15    5    3    8    3    4]\n",
      " [  18   27  260   11    0    2    2    9]\n",
      " [   6   11   17   41    0    2    0    3]\n",
      " [   0    3    0    0  386    0    0    0]\n",
      " [   3    4    3    0    0   24    0    1]\n",
      " [   1    3    5    0    0    0   27    0]\n",
      " [   8    7   13    8    0    3    0   36]]\n"
     ]
    }
   ],
   "source": [
    "# you do NOT need to rerun this cell\n",
    "f1_test, acc_test, conf = evaluate(model=model, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66f3709",
   "metadata": {},
   "source": [
    "---\n",
    "Let's have a look at the confusion matrix : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313520af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(plt.imread(os.path.join(model_path, \"confusion_matrix.png\")))\n",
    "plt.axis('off')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cd2e36",
   "metadata": {},
   "source": [
    "---\n",
    "We can have a look at some training metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7894e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "val_loss_df   = pd.read_csv(os.path.join(model_path, \"val_loss.csv\"))\n",
    "train_loss_df = pd.read_csv(os.path.join(model_path, \"train_loss.csv\"))\n",
    "val_acc_df    = pd.read_csv(os.path.join(model_path, \"val_acc.csv\"))\n",
    "train_acc_df  = pd.read_csv(os.path.join(model_path, \"train_acc.csv\"))\n",
    "\n",
    "# extract metrics\n",
    "train_loss = train_loss_df['train_loss'].values\n",
    "val_loss   = val_loss_df['val_loss'].values\n",
    "train_acc  = train_acc_df['train_acc'].values\n",
    "val_acc    = val_acc_df['val_acc'].values\n",
    "epochs     = range(1, len(train_loss) + 1)\n",
    "\n",
    "# create figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\n",
    "\n",
    "# plot loss\n",
    "ax1.plot(epochs, train_loss, label='Train Loss', linewidth=2)\n",
    "ax1.plot(epochs, val_loss,   label='Val Loss',   linewidth=2)\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# plot accuracy\n",
    "ax2.plot(epochs, train_acc, label='Train Acc', linewidth=2)\n",
    "ax2.plot(epochs, val_acc,   label='Val Acc',   linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04bf2af",
   "metadata": {},
   "source": [
    "### Questions : \n",
    "\n",
    "Have a look on the dataset description, it is a balance dataset ? Is there a a class that is way more represented than the other ? if yes wich one ? Assume that the train/test split is random so the frequency in the test set is the same than in the whole dataset\n",
    "\n",
    "*Answer:*\n",
    "\n",
    "Identify a potential prediction issue regarding the balance of the dataset by looking at the confusion matrix (be precise).\n",
    "\n",
    "*Answer:*\n",
    "\n",
    "Comment the evaluation result, accuracy, f1 score and the confusion matrix ?\n",
    "\n",
    "*Answer:* \n",
    "\n",
    "Comment the learning curves ? the model is overfitting ?\n",
    "\n",
    "*Answer:* \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aad35e",
   "metadata": {},
   "source": [
    "---\n",
    "*Réservé pour corrections*\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Corrections: /5 </b>\n",
    "</div>\n",
    "\n",
    "Commentaires: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b167595",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's have a look at some examples : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b2a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.randint(0, 1400, size=12)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "for i, index in enumerate(indices):\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    \n",
    "    true_label, predicted_label, true_prob, predicted_prob = inference(model, index, test_dataset)       \n",
    "    img = to_plot(index, test_dataset)\n",
    "    \n",
    "    axes[row, col].imshow(img)\n",
    "    axes[row, col].axis('off')\n",
    "    if true_prob == predicted_prob:\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    axes[row, col].set_title(\n",
    "        f\"True label: {true_label} with proba = {true_prob:.2f}\\nPred label: {predicted_label} with proba = {predicted_prob:.2f}\",\n",
    "        color=color,\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff5007",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now let's have a look at the model architecture, it is fine tune efficient net model b3 model. In the next cell we can see all the layers name and it's architecture. You can look there to have layer name information that you will need later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4755e085",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- **Q1:**. When looking at the images and the model’s predictions, what do you think might cause misclassifications? Can you identify important regions in the images that may affect the model’s decision?\n",
    "\n",
    "- **Q2:** As you can see, interpreting medical images is not always straightforward. In your own words, why is it important to have domain expertise when analyzing XAI results?\n",
    "\n",
    "- **Q3:** Can you briefly describe the structure of the model (an efficiennet model)? You don’t need to be exhaustive—just explain the main parts and how the model works in general.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f48cc9",
   "metadata": {},
   "source": [
    "---\n",
    "*Réservé pour corrections*\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Corrections: /4 </b>\n",
    "</div>\n",
    "\n",
    "Commentaires: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d134e81",
   "metadata": {},
   "source": [
    "## 1. PostHoc Explanations using Captum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d65aa2a",
   "metadata": {},
   "source": [
    "- To make predictions with a neural network, the data input is passed through many layers of multiplication with the learned weights and through non-linear transformations. A single prediction can involve millions of mathematical operations. There is no chance that we humans can follow the exact mapping from data input to prediction.\n",
    "\n",
    "- Some XAI posthoc methods can help to interpate a neural network result and we will parcours some of them in this section. To do so we will use the [**captum package**](https://captum.ai/)  that put together multiple xai methods including some for vision task for pytorch model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0921607",
   "metadata": {},
   "source": [
    "### Grad-CAM Exercise\n",
    "\n",
    "- Grad-CAM is an XAI method that highlights the regions most important to a convolutional neural network’s decision. We provide helper functions to compute and plot Grad-CAM for any input tensor, its label, and a chosen convolutional layer (typically one near the end of the network for best spatial resolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc84c5e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apply_gradcam(model, input_tensor, target_class, layer):\n",
    "    model.eval()\n",
    "    gradcam = LayerGradCam(model, layer)\n",
    "    attr = gradcam.attribute(input_tensor, target=target_class)\n",
    "    up = F.interpolate(attr, size=input_tensor.shape[2:], mode='bilinear')\n",
    "    up = up.squeeze().cpu().detach().numpy()\n",
    "    up = (up - up.min()) / (up.max() - up.min() + 1e-8)  # Normalize to [0, 1]\n",
    "    return up\n",
    "\n",
    "def overlay_heatmap(input_tensor, attribution, alpha=0.5):\n",
    "    img = to_pil_image(input_tensor.squeeze().cpu())\n",
    "\n",
    "    if torch.is_tensor(attribution):\n",
    "        hm = attribution.squeeze().cpu().numpy()\n",
    "    else:\n",
    "        hm = attribution\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis(\"off\")\n",
    "    heatmap = axes[1].imshow(hm, cmap=\"jet\", alpha=alpha)\n",
    "    axes[1].imshow(img, alpha=1 - alpha)\n",
    "    axes[1].set_title(\"Attribution\")\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    # Add color bar\n",
    "    cbar = fig.colorbar(heatmap, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(\"Attribution Intensity\", rotation=270, labelpad=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf3fef",
   "metadata": {},
   "source": [
    "Let's see the results on a random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1234 # random index of the test set\n",
    "img_tensor, class_idx = test_dataset[idx]\n",
    "img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "class_idx = class_idx.argmax().item()\n",
    "\n",
    "layer = model._conv_head\n",
    "\n",
    "gc_attr = apply_gradcam(model, img_tensor, class_idx, layer)\n",
    "overlay_heatmap(img_tensor, gc_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba9af70",
   "metadata": {},
   "source": [
    "### Grad-CAM Exploration\n",
    "\n",
    "1. Pick **one image** from the test set (any index you like).\n",
    "\n",
    "2. Choose:\n",
    "   - A **target convolutional layer** (usually one near the end of the model).\n",
    "   - A **target class** to explain (you can use the predicted class or try another one).\n",
    "\n",
    "3. Run **Grad-CAM** on that image.\n",
    "\n",
    "4. Then answer:\n",
    "   - Why did you choose this layer and class?\n",
    "   - Which parts of the image are highlighted in the heat-map?\n",
    "   - Why do you think the model focused on those regions?\n",
    "\n",
    "If you need help checking labels or predictions, you can use these small helpers function:\n",
    "\n",
    "```python\n",
    "from scripts.helpers import idx_to_predicted_label, idx_to_true_label, idx_to_proba\n",
    "``` \n",
    "\n",
    "For example : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b34008",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx_to_predicted_label(303, test_dataset, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606e6efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx_to_true_label(303, test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx_to_proba(303, test_dataset, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1f5d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2626bca3",
   "metadata": {},
   "source": [
    "*Answers* : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a1c20",
   "metadata": {},
   "source": [
    "---\n",
    "*Réservé pour corrections*\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Corrections: /5 </b>\n",
    "</div>\n",
    "\n",
    "Commentaires: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b054f4a",
   "metadata": {},
   "source": [
    "## Saliency and Occlusion\n",
    "\n",
    "Let's use others explainers from the Captum package, saliency map and occlusion. \n",
    "\n",
    "Your task : \n",
    "\n",
    "1. **Load the image** at index `836` from the test set.  \n",
    "   Run the model on it and print the output probabilities.\n",
    "\n",
    "2. **Answer the following questions**:  \n",
    "   - What is the **true label** of this image?  \n",
    "   - What is the **predicted label** by the model?\n",
    "\n",
    "3. **Use two explanation methods (no Grad-CAM)**:\n",
    "\n",
    "   Saliency (Vanilla Gradient)\n",
    "   - This method shows which pixels have the biggest effect on the model's prediction.\n",
    "   - Brighter areas = more important for the predicted class.\n",
    "   - **Q1:** Which part of the image is most highlighted in the saliency map?\n",
    "\n",
    "   Occlusion\n",
    "   - This method hides small parts of the image with a grey patch and checks how the prediction changes.\n",
    "   - If hiding a patch drops the score a lot → that patch was important.\n",
    "   - Use a patch size of `15 x 15` and stride `8`.\n",
    "   - **Q2:** Are the important areas bigger or smaller than in the saliency map?\n",
    "\n",
    "    **Plot the results** \n",
    "\n",
    "4. **Compare the two methods**\n",
    "   - **Q3:** Do both explanations highlight the same areas?  \n",
    "   - **Q4:** Which explanation do you find more useful, and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bcac86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea42cdc7",
   "metadata": {},
   "source": [
    "*Answers :*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8d3d5",
   "metadata": {},
   "source": [
    "---\n",
    "*Réservé pour corrections*\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Corrections: 8 </b>\n",
    "</div>\n",
    "\n",
    "Commentaires: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282b2d8",
   "metadata": {},
   "source": [
    "## 2. Undersand model's behaivour \n",
    "\n",
    "### Question – Why does the model confuse MEL and NV?\n",
    "\n",
    "- From the confusion matrix, we observe that the model frequently confuses **MEL** (melanoma) with **NV** (nevus).  \n",
    "\n",
    "- Despite having many examples of MEL, it is correctly predicted only about 65% of the time.\n",
    "\n",
    "- A significant portion of the misclassified examples comes from the confusion between MEL and NV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(plt.imread(os.path.join(model_path, \"confusion_matrix.png\")))\n",
    "plt.axis('off')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e581e5",
   "metadata": {},
   "source": [
    "We will try to understand why the model is mixing these two classes using XAI posthoc methods.\n",
    "\n",
    "You can use the next function to find example of MEL and NEV good and wrong predictions \n",
    "---\n",
    "\n",
    "1. Choose at least **one image from each group** above.\n",
    "2. Use at least **two different Captum methods** (e.g. GradCAM, Saliency, Occlusion, GuidedGradCam, GradientSHAP).\n",
    "3. For each image:\n",
    "   - Show the original image and the explanation map.\n",
    "   - Look at what parts of the image the model is using.\n",
    "   - Is the model focusing on the lesion or the background?\n",
    "\n",
    "4. **Write a short explanation**:\n",
    "   - What do you think causes the confusion between MEL and NV?\n",
    "   - Do some lesions look similar?\n",
    "   - Do some maps highlight the wrong areas?\n",
    "\n",
    "Try to make a hypothesis from what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed9d0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_examples(dataset, model, device, seed=42, max_trials=2000):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    matches = {\n",
    "        \"MEL-MEL\": None,\n",
    "        \"NV-NV\": None,\n",
    "        \"NV-MEL\": None,\n",
    "        \"MEL-NV\": None\n",
    "    }\n",
    "\n",
    "    for idx in indices[:max_trials]:\n",
    "        img, label = dataset[idx]\n",
    "        true_name = label_to_names(label)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(img.unsqueeze(0).to(device))\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            pred_idx = probs.argmax().item()\n",
    "            pred_name = class_names[pred_idx]\n",
    "\n",
    "        key = f\"{true_name}-{pred_name}\"\n",
    "        if key in matches and matches[key] is None:\n",
    "            matches[key] = idx\n",
    "\n",
    "        if all(v is not None for v in matches.values()):\n",
    "            break\n",
    "\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a51c3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a428c744",
   "metadata": {},
   "source": [
    "*Answers :*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9497260",
   "metadata": {},
   "source": [
    "---\n",
    "*Réservé pour corrections*\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Corrections: /8 </b>\n",
    "</div>\n",
    "\n",
    "Commentaires: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d45cd09",
   "metadata": {},
   "source": [
    "FIN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
